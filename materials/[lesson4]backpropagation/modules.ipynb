{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Module** - это абстрактный класс определяющий необходимые для нейронной сети методы. В этой части не нужно ничего писать, просто читайте комментарии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    def __init__ (self):\n",
    "        self.output = None\n",
    "        self.gradInput = None\n",
    "        self.training = True\n",
    "    \"\"\"\n",
    "    В целом, Module это черный ящик, который принимает что-то на вход и\n",
    "    выдает какой-то результат. Это результат применения метода \"forward\":\n",
    "        \n",
    "        output = module.forward(input)\n",
    "    \n",
    "    Также этот черный ящик должен уметь по значеню dL/dOutput подсчитывать\n",
    "    производную функции ошибки по параметрам модели (dL/dW) и по входным данным\n",
    "    (dL/dInput)\n",
    "    \n",
    "        gradInput = module.backward(input, gradOutput)\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Вычислает по данному input соответствующий output, сохраняя его в одноименном\n",
    "        поле себя.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Пример для тождественной функции: \n",
    "        \n",
    "        # self.output = input\n",
    "        # return self.output\n",
    "        \n",
    "        assert False, 'Implementation error'\n",
    "\n",
    "    def backward(self, input, gradOutput):\n",
    "        \"\"\"\n",
    "        Делает шаг обратного распространения ошибки.\n",
    "        \n",
    "        Включает в себя:\n",
    "        1) подсчет градиента функции ошибки по входу (для выполнения\n",
    "           шага обратного распространения ошибки предыдущего слоя)\n",
    "        2) подсчет градиента функции ошибки по параметрам (для \n",
    "           выполнения шага градиентного спуска)\n",
    "        \"\"\"\n",
    "        self.updateGradInput(input, gradOutput)\n",
    "        self.accGradParameters(input, gradOutput)\n",
    "        return self.gradInput\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        \"\"\"\n",
    "        Подсчитывает градиент функции ошибки по входу, сохраняет результат\n",
    "        в поле gradInput. Размерность полученного градиента всегда строго совпадает\n",
    "        с размерностью входа.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Для тождественной функции:\n",
    "        \n",
    "        # self.gradInput = gradOutput \n",
    "        # return self.gradInput\n",
    "        \n",
    "        assert False, 'Implementation error'\n",
    "    \n",
    "    def accGradParameters(self, input, gradOutput):\n",
    "        \"\"\"\n",
    "        Подсчитывает градиент функции потерь по параметрам\n",
    "        \n",
    "        Не нужно реализовывать если у модуля нет параметров (ReLU)\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def zeroGradParameters(self): \n",
    "        \"\"\"\n",
    "        Обнуляет градиент\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def getParameters(self):\n",
    "        \"\"\"\n",
    "        Возвращает список подгоняемых (trainable) параметров.\n",
    "        \n",
    "        Если таковых нет, то возвращает пустой список.\n",
    "        \"\"\"\n",
    "        return []\n",
    "        \n",
    "    def getGradParameters(self):\n",
    "        \"\"\"\n",
    "        Возвращает список градиентов функции потерь по подгоняемым параметрам.\n",
    "        \n",
    "        Если таковых нет, то возвращает пустой список.\n",
    "        \"\"\"\n",
    "        return []\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Переводит модуль в режим обучение. Влияет только на дропаут и batchNorm.\n",
    "        \"\"\"\n",
    "        self.training = True\n",
    "    \n",
    "    def eval(self):\n",
    "        \"\"\"\n",
    "        Отключает режим обучения. Влияет только на дропаут и batchNorm.\n",
    "        \"\"\"\n",
    "        self.training = False\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Выводит имя модуля.\n",
    "        \"\"\"\n",
    "        return \"Module\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define** a forward and backward pass procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    \"\"\"\n",
    "         Это контейнер, который для данного входа последовательно применяет\n",
    "         все переданные в него модули (выход предущего модуля - вход следующего)\n",
    "         \n",
    "         Итоговое значение будет являться выходом данного контейнера.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__ (self):\n",
    "        super(Sequential, self).__init__()\n",
    "        self.modules = []\n",
    "   \n",
    "    def add(self, module):\n",
    "        \"\"\"\n",
    "        Добавляет модуль в контейнер.\n",
    "        \"\"\"\n",
    "        self.modules.append(module)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Вычислает по данному input соответствующий output, сохраняя его в одноименном\n",
    "        поле себя.\n",
    "        \n",
    "            y_0    = module[0].forward(input)\n",
    "            y_1    = module[1].forward(y_0)\n",
    "            ...\n",
    "            output = module[n-1].forward(y_{n-2})\n",
    "        \"\"\"\n",
    "\n",
    "        self.inputs = []\n",
    "        last_output = input\n",
    "        \n",
    "        for i, mod in enumerate(self.modules):\n",
    "            self.inputs.append(last_output)\n",
    "            last_output = mod.forward(self.inputs[-1])\n",
    "            \n",
    "        self.output = last_output\n",
    "                \n",
    "        return self.output\n",
    "\n",
    "    def backward(self, input, gradOutput):\n",
    "        \"\"\"\n",
    "        Реализует backward pass:\n",
    "            \n",
    "            g_{n-1} = module[n-1].backward(y_{n-2}, gradOutput)\n",
    "            g_{n-2} = module[n-2].backward(y_{n-3}, g_{n-1})\n",
    "            ...\n",
    "            g_1 = module[1].backward(y_0, g_2)   \n",
    "            gradInput = module[0].backward(input, g_1)   \n",
    "        \n",
    "        \"\"\"\n",
    "        yinp = self.m_inputs[::-1]\n",
    "        t = gradOutput\n",
    "        \n",
    "        for i, mod in enumerate(self.modules[::-1]):\n",
    "            t = mod.backward(yinp[i], t)\n",
    "                \n",
    "        self.gradInput = t\n",
    "        return self.gradInput\n",
    "      \n",
    "\n",
    "    def zeroGradParameters(self): \n",
    "        for module in self.modules:\n",
    "            module.zeroGradParameters()\n",
    "    \n",
    "    def getParameters(self):\n",
    "        \"\"\"\n",
    "        Возвращает список списков параметров каждого модуля.\n",
    "        \"\"\"\n",
    "        return [x.getParameters() for x in self.modules]\n",
    "    \n",
    "    def getGradParameters(self):\n",
    "        \"\"\"\n",
    "        Возвращает список списков градиентов функции потерь по параметрам каждого модуля.\n",
    "        \"\"\"\n",
    "        return [x.getGradParameters() for x in self.modules]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        string = \"\".join([str(x) + '\\n' for x in self.modules])\n",
    "        return string\n",
    "    \n",
    "    def __getitem__(self,x):\n",
    "        return self.modules.__getitem__(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- input:   **`batch_size x n_feats1`**\n",
    "- output: **`batch_size x n_feats2`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    \"\"\"\n",
    "    Модуль выполняющий линейное преобразование над входом. Также известен\n",
    "    как полносвязный слой.\n",
    "    \n",
    "    Модуль должен принимать на вход матрицу размерности (n_samples, n_feature).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super(Linear, self).__init__()\n",
    "       \n",
    "        # This is a nice initialization\n",
    "        stdv = 1./np.sqrt(n_in)\n",
    "        self.W = np.random.uniform(-stdv, stdv, size = (n_out, n_in))\n",
    "        \n",
    "        self.gradW = np.zeros_like(self.W)\n",
    "        \n",
    "    def dumb_forward(self, input):\n",
    "        '''\n",
    "        Считает выход линейного слоя по данному входу, сохраняет результат в self.output\n",
    "        и вовзращает self.output.\n",
    "        output[object_idx, out_neuron_idx] = \n",
    "        \\sum_{in_neuron_idx} input[object_idx,n] * w_[out_neuron_idx, in_neuron_idx]\n",
    "        '''\n",
    "        n_obj = len(input)\n",
    "        n_out, n_in = self.W.shape\n",
    "        self.output = np.zeros((n_obj, n_out))\n",
    "        for i_obj in range(n_obj):\n",
    "            for i_out in range(n_out):\n",
    "                for i_in in range(n_in):\n",
    "                    self.output[i_obj, i_out] += self.W[i_out][i_in] * input[i_obj][i_in]\n",
    "                \n",
    "        return self.output\n",
    "    \n",
    "    def forward(self, input1):\n",
    "        self.output = (self.W @ input1.T).T\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        assert False, \"Implementation error\"\n",
    "\n",
    "    def accGradParameters(self, input, gradOutput):\n",
    "        assert False, \"Implementation error\"\n",
    "    \n",
    "    def zeroGradParameters(self):\n",
    "        self.gradW.fill(0)\n",
    "        self.gradb.fill(0)\n",
    "        \n",
    "    def getParameters(self):\n",
    "        return [self.W]\n",
    "    \n",
    "    def getGradParameters(self):\n",
    "        return [self.gradW]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        s = self.W.shape\n",
    "        q = 'Linear %d -> %d' %(s[1],s[0])\n",
    "        return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one is probably the hardest but as others only takes 5 lines of code in total. \n",
    "- input:   **`batch_size x n_feats`**\n",
    "- output: **`batch_size x n_feats`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from time import time, sleep\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_size = 4\n",
    "num_classes = 3\n",
    "num_inputs = 5\n",
    "\n",
    "def init_linear():\n",
    "    np.random.seed(0)\n",
    "    return Linear(input_size, num_classes)\n",
    "\n",
    "def init_toy_data():\n",
    "    np.random.seed(1)\n",
    "    X = 10 * np.random.randn(num_inputs, input_size)\n",
    "    y = np.array([0, 1, 2, 2, 1])\n",
    "    return X, y\n",
    "\n",
    "linear = init_linear()\n",
    "X, y = init_toy_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your output:\n",
      "[[ -1.54788446  -6.00658097   6.39369587]\n",
      " [ -3.07885716  -8.08969546  11.56531411]\n",
      " [  0.19697038  -9.59100515   5.43998673]\n",
      " [ -0.31239372  -5.33085678   1.94239605]\n",
      " [ -1.66825993   1.10786278   0.51528511]]\n",
      "\n",
      "correct output:\n",
      "[[ -1.54788446  -6.00658097   6.39369587]\n",
      " [ -3.07885716  -8.08969546  11.56531411]\n",
      " [  0.19697038  -9.59100515   5.43998673]\n",
      " [ -0.31239372  -5.33085678   1.94239605]\n",
      " [ -1.66825993   1.10786278   0.51528511]]\n",
      "\n",
      "Difference between your scores and correct scores:\n",
      "1.02925382778e-08\n"
     ]
    }
   ],
   "source": [
    "output = linear.forward(X)\n",
    "print('Your output:')\n",
    "print(output)\n",
    "print()\n",
    "print('correct output:')\n",
    "correct_output = np.asarray(\n",
    "[[ -1.54788446,  -6.00658097,   6.39369587],\n",
    " [ -3.07885716,  -8.08969546,  11.56531411],\n",
    " [  0.19697038,  -9.59100515,   5.43998673],\n",
    " [ -0.31239372,  -5.33085678,  1.94239605],\n",
    " [ -1.66825993,   1.10786278,   0.51528511]])\n",
    "print(correct_output)\n",
    "print()\n",
    "\n",
    "# The difference should be very small. We get < 1e-7\n",
    "print('Difference between your scores and correct scores:')\n",
    "print(rel_error(output, correct_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функции активации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU **Rectified Linear Unit** реализован за вас: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "         super(ReLU, self).__init__()\n",
    "    \n",
    "    def updateOutput(self, input):\n",
    "        self.output = np.maximum(input, 0)\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = np.multiply(gradOutput , input > 0)\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"ReLU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sigmoid(Module):\n",
    "    def __init__(self):\n",
    "        super(Sigmoid, self).__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.output = 1/(1 + np.exp(-input))\n",
    "        return  self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = self.output * (1 - self.output)\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Sigmoid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequential = Sequential()\n",
    "linear_small = Linear(n_in=2, n_out=2)\n",
    "linear_small.W = np.array([[0.9, 0.3], [0.2, 0.8]])\n",
    "linear_small.b = np.zeros(2)\n",
    "sequential.add(linear_small)\n",
    "sequential.add(Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your output:\n",
      "[[ 0.7407749   0.64565631]]\n",
      "\n",
      "correct output:\n",
      "[[ 0.7408  0.6457]]\n",
      "\n",
      "Difference between your scores and correct scores:\n",
      "3.38355680721e-05\n"
     ]
    }
   ],
   "source": [
    "X_small = np.array([[1, 0.5]])\n",
    "output_small = sequential.forward(X_small)\n",
    "print('Your output:')\n",
    "print(output_small)\n",
    "print()\n",
    "print('correct output:')\n",
    "correct_output_small = np.asarray([[0.7408, 0.6457]])\n",
    "print(correct_output_small)\n",
    "print()\n",
    "\n",
    "print('Difference between your scores and correct scores:')\n",
    "print(rel_error(output_small, correct_output_small))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criterions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функции потерь. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Criterion(object):\n",
    "    def __init__ (self):\n",
    "        self.output = None\n",
    "        self.gradInput = None\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        \"\"\"\n",
    "            Считает ошибку, сохраняет ее и возвращает.\n",
    "        \"\"\"\n",
    "        assert False, \"Implementation error\"\n",
    "\n",
    "    def backward(self, input, target):\n",
    "        \"\"\"\n",
    "            Считает градиент ошибки по input, сохраняет его в gradInput\n",
    "            и возвращает.\n",
    "        \"\"\"\n",
    "        self.updateGradInput(input, target)\n",
    "        return self.gradInput\n",
    "\n",
    "    def updateGradInput(self, input, target):\n",
    "        \"\"\"\n",
    "        Function to override.\n",
    "        \"\"\"\n",
    "        assert False, \"Implementation error\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Выводит название функции потерь в человекочитаемом виде.\n",
    "        \"\"\"\n",
    "        return \"Criterion\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **MSECriterion**, which is basic L2 norm usually used for regression, is implemented here for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MSECriterion(Criterion):\n",
    "    def __init__(self):\n",
    "        super(MSECriterion, self).__init__()\n",
    "        \n",
    "    def updateOutput(self, input, target):   \n",
    "        self.output = np.sum(np.power(input - target,2)) / input.shape[0]\n",
    "        return self.output \n",
    " \n",
    "    def updateGradInput(self, input, target):\n",
    "        self.gradInput  = (input - target) * 2 / input.shape[0]\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"MSECriterion\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
